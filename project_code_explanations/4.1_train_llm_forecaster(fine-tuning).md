# Time-Series Forecasting with GPT-2

This project fine-tunes a GPT-2 model for time-series forecasting using prompt-completion pairs.

## Features
- Fine-tunes GPT-2 on custom time-series data.
- Supports training and validation datasets.
- Saves the fine-tuned model for future use.

## Requirements
- Python 3.13+
- PyTorch
- Transformers
- Pandas
- Jupyter Notebook

## Setup
1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd <repository-folder>
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Prepare your data:
   - Place your training and validation CSV files in the `data/llm_preprocessed/` directory.
   - Each file should have `prompt` and `completion` columns.

## Usage
1. Open the Jupyter Notebook:
   ```bash
   jupyter notebook scripts/train_llm_forecaster.ipynb
   ```

2. Run the cells in order:
   - Load the model and tokenizer.
   - Prepare datasets.
   - Fine-tune the model.

3. The fine-tuned model will be saved in the `models/llm_forecaster/` directory.

## Training Arguments
The training process uses the following arguments:
- **Epochs**: 5
- **Batch Size**: 8
- **Learning Rate**: 5e-5
- **Weight Decay**: 0.01
- **Mixed Precision**: Enabled if GPU is available.

## Output
- The fine-tuned model is saved in the `models/llm_forecaster/` directory.
- Logs are stored in `models/llm_forecaster/logs/`.

## Notes
- Ensure your data is preprocessed and formatted correctly.
- Update the `MODEL_NAME` in the notebook to experiment with different pre-trained models.

## References
- [Transformers Documentation](https://huggingface.co/docs/transformers/)
- [PyTorch Documentation](https://pytorch.org/docs/)