{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "üìò Step 3: Transformer-Based Time Series Forecasting (Informer)\n",
    "\n",
    "We‚Äôll use the Hugging Face ü§ó implementation of a pretrained Transformer model adapted for time series forecasting.\n",
    "\n",
    "Since Transformer time series models are not in Hugging Face‚Äôs core repo, we‚Äôll use the open-source repo: Informer2020 (by Haoyi Zhou)\n",
    "\n",
    "But first, here‚Äôs a simplified PyTorch-based custom Transformer that you can use as a baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üì¶ Imports\n",
    "import torch\n",
    "import torch.nn as nnimport torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pdimport pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pltimport matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# üìÇ Load the dataset\n",
    "df = pd.read_csv(\"../data/processed/etth1_processed.csv\", parse_dates=[\"date\"], index_col=\"date\")\n",
    "target = df[\"OT\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üß© Dataset\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, series, window_size):\n",
    "        self.series = torch.tensor(series, dtype=torch.float32)\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.series) - self.window_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.series[idx:idx + self.window_size]\n",
    "        y = self.series[idx + self.window_size]\n",
    "        return x.unsqueeze(1), y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# üîÑ Create Dataloader\n",
    "window_size = 48\n",
    "batch_size = 32\n",
    "dataset = TimeSeriesDataset(target, window_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ü§ñ Transformer Encoder Model (Simplified)\n",
    "class TransformerForecast(nn.Module):\n",
    "    def __init__(self, input_size=1, d_model=64, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_size, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # [batch, seq_len, d_model]\n",
    "        x = self.transformer(x)\n",
    "        return self.output(x[:, -1, :])  # last token for prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Training Loop\n",
    "model = TransformerForecast().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device).unsqueeze(1)\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üìà Plot Predictions\n",
    "model.eval()\n",
    "x_test, y_test = next(iter(dataloader))\n",
    "x_test = x_test.to(device)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_test).cpu().numpy()\n",
    "    y_true = y_test.numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_true[:50], label=\"True\")\n",
    "plt.plot(y_pred[:50], label=\"Predicted\")\n",
    "plt.title(\"üîÆ Transformer Forecast vs True\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "‚úÖ Summary\n",
    "\t‚Ä¢\t‚úÖ This model uses a basic Transformer Encoder for prediction.\n",
    "\t‚Ä¢\tüß† You can later replace it with more powerful variants: Informer, Autoformer, PatchTST.\n",
    "\t‚Ä¢\tüîß Easy to integrate with existing dataset and preprocessing steps.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
