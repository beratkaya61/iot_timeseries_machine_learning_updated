{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96a8ea15",
   "metadata": {},
   "source": [
    "utilized from optuna for hyper parameter optimization technic.\n",
    "\n",
    "and enhance performance of lstm model via best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f2f82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import os\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "import json\n",
    "import sys\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c531f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to the Python path\n",
    "sys.path.append('../') \n",
    "from models.lstm_forecast_model import LSTMForecast\n",
    "from models.timeseries_dataset_class import TimeSeriesDataset\n",
    "\n",
    "# ‚úÖ Enable GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SMAPE calculation\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return 100 * np.mean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e1a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "def evaluate_model(model, dataloader, title, plot_name, writer=None, step=0):\n",
    "    model.eval()\n",
    "    all_preds, all_trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            pred = model(x).cpu().numpy()\n",
    "            all_preds.append(pred)\n",
    "            all_trues.append(y.numpy())\n",
    "    predictions = np.concatenate(all_preds)\n",
    "    truths = np.concatenate(all_trues)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(truths[:100], label=\"True\")\n",
    "    plt.plot(predictions[:100], label=\"Predicted\")\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    os.makedirs(\"../../outputs/LSTM_Model\", exist_ok=True)\n",
    "    plot_path = f\"../../outputs/LSTM_Model/{plot_name}\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n",
    "    print(f\"‚úÖ Plot saved to outputs/{plot_name}\")\n",
    "\n",
    "    mse = mean_squared_error(truths, predictions)\n",
    "    mae = mean_absolute_error(truths, predictions)\n",
    "    smape_val = smape(truths, predictions)\n",
    "    print(f\"üìä {title} Metrics:\")\n",
    "    print(f\" - MSE   : {mse:.6f}\")\n",
    "    print(f\" - MAE   : {mae:.6f}\")\n",
    "    print(f\" - SMAPE : {smape_val:.2f}%\")\n",
    "\n",
    "    if writer:\n",
    "        writer.add_scalar(\"Metrics/MSE\", mse, step)\n",
    "        writer.add_scalar(\"Metrics/MAE\", mae, step)\n",
    "        writer.add_scalar(\"Metrics/SMAPE\", smape_val, step)\n",
    "\n",
    "    return predictions, truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e91a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly scoring based on residual thresholding\n",
    "def anomaly_score(y_true, y_pred, threshold=3.0):\n",
    "\n",
    "    # Ensure y_true and y_pred are 1D arrays with flattening or squeezing\n",
    "    if y_true.ndim > 1:\n",
    "        y_true = y_true.flatten()\n",
    "    if y_pred.ndim > 1:\n",
    "        y_pred = y_pred.flatten()\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"y_true and y_pred must have the same shape for anomaly detection.\")\n",
    "\n",
    "    residual = np.abs(y_true - y_pred)\n",
    "    mean = np.mean(residual)\n",
    "    std = np.std(residual)\n",
    "    z_score = (residual - mean) / std\n",
    "    anomalies = np.where(np.abs(z_score) > threshold)[0]\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(y_true, label='True')\n",
    "    plt.plot(y_pred, label='Predicted')\n",
    "    plt.scatter(anomalies, y_true[anomalies], color='red', label='Anomalies')\n",
    "    plt.title(\"Anomaly Detection Result via z-score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"../../outputs/LSTM_Model/anomaly_detection_visual_z_score.png\")\n",
    "    plt.show()\n",
    "    print(\"‚úÖ Anomaly plot saved to anomaly_detection_visual_z_score.png\")\n",
    "\n",
    "    # Automatic anomaly description\n",
    "    for idx in anomalies[:5]:\n",
    "        true_val = float(np.ravel(y_true[idx])[0])\n",
    "        pred_val = float(np.ravel(y_pred[idx])[0])\n",
    "        z_val = float(np.ravel(z_score[idx])[0])\n",
    "        print(f\"‚ö†Ô∏è Anomaly at index {idx} ‚Üí True: {true_val:.2f}, Predicted: {pred_val:.2f}, Z-score: {z_val:.2f}\")\n",
    "        \n",
    "    return anomalies, z_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff54bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest (optional advanced detector)\n",
    "def isolation_forest_detection(residuals):\n",
    "    iso = IsolationForest(contamination=0.01)\n",
    "    preds = iso.fit_predict(residuals.reshape(-1, 1))\n",
    "    anomalies = np.where(preds == -1)[0]\n",
    "    return anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af9b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Isolation Forest Visualization\n",
    "def visualize_isolation_forest_anomalies(y_true, y_pred, iso_anomalies):\n",
    "    # Filter out-of-bounds indices\n",
    "    iso_anomalies = [idx for idx in iso_anomalies if idx < len(y_true)]\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(y_true, label=\"True\")\n",
    "    plt.plot(y_pred, label=\"Predicted\")\n",
    "\n",
    "    plt.scatter(iso_anomalies, y_true[iso_anomalies], color=\"green\", label=\"ISO Anomalies\")\n",
    "\n",
    "    plt.title(\"Isolation Forest - Anomaly Detection\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    iso_plot_path = \"../../outputs/LSTM_Model/isolation_forest_anomaly_plot.png\"\n",
    "    plt.savefig(iso_plot_path)\n",
    "    plt.show()\n",
    "    print(f\"‚úÖ Isolation Forest anomaly plot saved to: {iso_plot_path}\")\n",
    "\n",
    "    for idx in iso_anomalies[:5]:\n",
    "        true_val = float(np.ravel(y_true[idx])[0])\n",
    "        pred_val = float(np.ravel(y_pred[idx])[0])\n",
    "        residual_val = float(np.ravel(np.abs(y_true[idx] - y_pred[idx]))[0])\n",
    "        print(f\"üå™Ô∏è ISO Anomaly at index {idx} ‚Üí True: {true_val:.2f}, Predicted: {pred_val:.2f}, Residual: {residual_val:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate with early stopping\n",
    "def train_and_evaluate(model, train_loader, val_loader, optimizer, loss_fn, epochs=50, early_stop=5, writer=SummaryWriter(log_dir=\"../runs/LSTM_Model/train_and_eval\")):\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device).unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(model(x), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        # writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device).unsqueeze(1)\n",
    "                val_loss += loss_fn(model(x), y).item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        # writer.add_scalar(\"Loss/val\", avg_val_loss, epoch)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= early_stop:\n",
    "                break\n",
    "    # writer.close()\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27329910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    window_size = trial.suggest_int(\"window_size\", 24, 512)\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "    train_df = pd.read_csv(\"../data/processed/etth1_train.csv\", parse_dates=[\"date\"], index_col=\"date\")\n",
    "    val_df = pd.read_csv(\"../data/processed/etth1_val.csv\", parse_dates=[\"date\"], index_col=\"date\")\n",
    "    train_target = train_df[\"OT\"].values\n",
    "    val_target = val_df[\"OT\"].values\n",
    "\n",
    "    train_loader = DataLoader(TimeSeriesDataset(train_target, window_size), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TimeSeriesDataset(val_target, window_size), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = LSTMForecast(hidden_size=hidden_size, num_layers=num_layers, dropout=dropout).to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    return train_and_evaluate(model, train_loader, val_loader, optimizer, loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d260346",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2031ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n‚úÖ Best Hyperparameters:\")\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79443d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save best hyperparameters\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "with open(\"../models/best_params_of_lstm_model.json\", \"w\") as f:\n",
    "    json.dump(study.best_trial.params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1413877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "# Retrain model with best parameters\n",
    "with open(\"../models/best_params_of_lstm_model.json\", \"r\") as f:\n",
    "    params = json.load(f)\n",
    "window_size = params[\"window_size\"]\n",
    "batch_size = params[\"batch_size\"]\n",
    "train_df = pd.read_csv(\"../data/processed/etth1_train.csv\", parse_dates=[\"date\"], index_col=\"date\")\n",
    "val_df = pd.read_csv(\"../data/processed/etth1_val.csv\", parse_dates=[\"date\"], index_col=\"date\")\n",
    "test_df = pd.read_csv(\"../data/processed/etth1_test.csv\", parse_dates=[\"date\"], index_col=\"date\")\n",
    "\n",
    "# use the best parameters to create DataLoaders\n",
    "train_loader = DataLoader(TimeSeriesDataset(train_df[\"OT\"].values, window_size), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TimeSeriesDataset(val_df[\"OT\"].values, window_size), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TimeSeriesDataset(test_df[\"OT\"].values, window_size), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = LSTMForecast(hidden_size=params[\"hidden_size\"], num_layers=params[\"num_layers\"], dropout=params[\"dropout\"]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32ec5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(model, train_loader, val_loader, optimizer, loss_fn)\n",
    "torch.save(model.state_dict(), \"../models/checkpoints/lstm_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e29b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "writer = SummaryWriter(log_dir=\"../runs/LSTM_Model/lstm_eval_final\")\n",
    "preds, trues = evaluate_model(model, val_loader, \"Validation Set Prediction vs Ground Truth\", \"lstm_model_val_predictions.png\", writer, step=0)\n",
    "preds_test, trues_test = evaluate_model(model, test_loader, \"Test Set Prediction vs Ground Truth\", \"lstm_model_test_predictions.png\", writer, step=1)\n",
    "\n",
    "# Anomaly detection with z-score\n",
    "z_score_anomalies, z_score = anomaly_score(trues_test, preds_test)\n",
    "print(f\"üîé Detected {len(z_score_anomalies)} anomalies in the test set\")\n",
    "\n",
    "# Save anomalies and z-scores\n",
    "np.save(\"../../outputs/LSTM_Model/z_score_anomaly_indices.npy\", z_score_anomalies)\n",
    "np.save(\"../../outputs/LSTM_Model/z_scores.npy\", z_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Forest anomaly detection\n",
    "\n",
    "# convert preds_test and trues_test to 1D array with squeeze or flatten\n",
    "\n",
    "if preds_test.ndim > 1:\n",
    "    preds_test = preds_test.squeeze()\n",
    "if trues_test.ndim > 1:\n",
    "    trues_test = trues_test.squeeze()\n",
    "\n",
    "residuals = np.abs(trues_test - preds_test)\n",
    "\n",
    "iso_anomalies = isolation_forest_detection(residuals)\n",
    "\n",
    "print(f\"üå™Ô∏è Detected {len(iso_anomalies)} anomalies using Isolation Forest\")\n",
    "\n",
    "visualize_isolation_forest_anomalies(trues_test, preds_test, iso_anomalies)\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb884a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both anomaly detection methods (e.g., IForest & Z-Score) in one plot\n",
    "\n",
    "# Ensure residuals is a NumPy array\n",
    "residuals_np = residuals if isinstance(residuals, np.ndarray) else residuals.values\n",
    "iso_anomalies_np = iso_anomalies if isinstance(iso_anomalies, np.ndarray) else iso_anomalies.values\n",
    "zscore_anomalies_np = z_score_anomalies if isinstance(z_score_anomalies, np.ndarray) else z_score_anomalies.values\n",
    "\n",
    "# Use np.arange for time steps\n",
    "time_steps = np.arange(len(residuals_np))\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(time_steps, residuals_np, label='Residuals', color='blue')\n",
    "\n",
    "# Plot isolation forest anomalies\n",
    "plt.scatter(time_steps[iso_anomalies_np], residuals_np[iso_anomalies_np],\n",
    "            color='red', marker='x', label='Isolation Forest',s=100)\n",
    "\n",
    "# Plot z-score anomalies\n",
    "plt.scatter(time_steps[zscore_anomalies_np], residuals_np[zscore_anomalies_np],\n",
    "            color='green', marker='o', label='Z-Score', facecolors='none',s=100)\n",
    "\n",
    "plt.title(\"Residuals with Anomalies Detected by Both Methods\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../../outputs/LSTM_Model/compare_anomaly_detection_methods.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3796c26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate prediction accuracy and anomaly detection metrics\n",
    "print(\"\\nüìä Final Evaluation Metrics:\")\n",
    "\n",
    "preds_test = preds_test.flatten()\n",
    "trues_test = trues_test.flatten()\n",
    "\n",
    "mse= mean_squared_error(trues_test, preds_test)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(trues_test, preds_test)\n",
    "r2 = r2_score(trues_test, preds_test)\n",
    "\n",
    "smape_val = smape(trues_test, preds_test)\n",
    "\n",
    "metrics_data = { \n",
    "    'Metric': ['MSE', 'RMSE', 'MAE', 'SMAPE (%)', 'R¬≤'],\n",
    "    'Score': [mse, rmse, mae, smape_val, r2]\n",
    "}\n",
    "\n",
    "# 'Anomalies_Z_Score','Anomalies_ISO_Forest' \n",
    "# len(z_score_anomalies), len(iso_anomalies)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "colors = ['skyblue', 'orange', 'limegreen', 'salmon', 'mediumpurple', 'lightcoral', 'gold']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.set_xticklabels(metrics_df['Metric'], rotation=45, ha='right')\n",
    "bars = ax.bar(metrics_df['Metric'], metrics_df['Score'], color=colors)\n",
    "\n",
    "# Annotate bars\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, yval + 0.01, f'{yval:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax.set_title('Evaluation Metrics for LSTM Model on Test Set', fontsize=14)\n",
    "ax.set_ylabel('Score')\n",
    "ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc1ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.zeros_like(residuals)\n",
    "y_true[iso_anomalies] = 1  # 1 = anomaly, 0 = normal\n",
    "\n",
    "#Use residuals as anomaly scores\n",
    "y_scores = residuals\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(f\" ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Random Guess\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve for LSTM Anomaly Detection (Pseudo Labels)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29a22af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make above operations for precision, recall and f1-score\n",
    "\n",
    "# print(\"trues_test :\", trues_test)\n",
    "\n",
    "# print(\"preds_test :\", preds_test)\n",
    "\n",
    "# pred_residuals = np.abs(trues_test - preds_test)\n",
    "\n",
    "# # Step 2: Create pseudo-ground-truth by thresholding true values\n",
    "# # Here we assume large changes in actual values mean real anomalies\n",
    "# true_residuals = np.abs(trues_test - np.mean(trues_test))\n",
    "# anomaly_threshold = np.percentile(true_residuals, 95)  # top 5% residuals = anomalies\n",
    "# true_binary_labels = (true_residuals > anomaly_threshold).astype(int)\n",
    "\n",
    "# # Step 3: ROC curve\n",
    "# fpr, tpr, thresholds = roc_curve(true_binary_labels, pred_residuals)\n",
    "# auc_score = roc_auc_score(true_binary_labels, pred_residuals)\n",
    "\n",
    "# # Step 4: Plot\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc_score:.4f})\", color='blue')\n",
    "# plt.plot([0, 1], [0, 1], 'k--', label='Random Baseline')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve for Unsupervised Anomaly Detection (ETTh1 - OT)')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print(f\"AUC Score: {auc_score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcef0fd",
   "metadata": {},
   "source": [
    "1. Inject synthetic anomalies into the OT column of ETTh1\n",
    "   - Why? Since the original dataset has no labels, you artificially inject anomalies (e.g., spikes, drops, out-of-distribution values) to create known positions of anomalies, i.e., y_true.\n",
    "\n",
    "2. Train your LSTM model on the original (clean) ETTh1\n",
    "   - This gives the model a sense of what \"normal\" time-series behavior looks like.\n",
    "\n",
    "3. Evaluate on the anomaly-injected version of the dataset\n",
    "   - Feed the injected dataset through the LSTM model to get predicted values.\n",
    "\n",
    "   - Calculate residuals (absolute difference between prediction and actual).\n",
    "\n",
    "   - These residuals serve as anomaly scores ‚Üí y_scores.\n",
    "\n",
    "4. Use the injected anomaly positions as ground-truth (y_true)\n",
    "   - The y_true is a binary array where injected anomaly indices are 1, all others 0.\n",
    "\n",
    "   - Use y_scores and y_true for:\n",
    "\n",
    "        - roc_auc_score\n",
    "\n",
    "        - precision_recall_curve\n",
    "\n",
    "        - f1_score, precision, recall\n",
    "\n",
    "        - confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "‚ÄúHow do I know that the injected data contains anomalies?‚Äù\n",
    "\n",
    "You create and control the anomalies yourself ‚Äî so you define where they are and what kind they are. These synthetic changes become your ground-truth anomaly labels (y_true). The types of anomalies can include:\n",
    "\n",
    "    -  Point anomalies: Inject random spikes or dips (e.g., multiply a value by 2x or set to 0)\n",
    "\n",
    "    -  Contextual anomalies: Break seasonality by shifting values only in specific time windows\n",
    "\n",
    "    -  Collective anomalies: Introduce a sequence of values that deviate from normal patterns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9562b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_anomalies(data, anomaly_fraction=0.02, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    n = len(data)\n",
    "    total_anomaly_count = int(anomaly_fraction * n)\n",
    "\n",
    "    # keep original data\n",
    "    original_data = data.copy()\n",
    "    anomaly_labels = np.zeros(n)\n",
    "    anomaly_types = np.array([\"normal\"] * n, dtype=object)\n",
    "\n",
    "    # Basic stats\n",
    "    data_mean = np.mean(data)\n",
    "    data_std = np.std(data)\n",
    "\n",
    "    # Allocate anomalies by expected impact size\n",
    "    # point: 1 point per injection\n",
    "    # contextual: 1 point per injection\n",
    "    # collective: 5 points per injection\n",
    "\n",
    "\n",
    "    # Anomaly types: KEYNOTE\n",
    "    # -  Point anomalies: Inject random spikes or dips (e.g., multiply a value by 2x or set to 0)\n",
    "    # -  Contextual anomalies: Break seasonality by shifting values only in specific time windows\n",
    "    # -  Collective anomalies: Introduce a sequence of values that deviate from normal patterns\n",
    "\n",
    "    point_budget = int(0.3 * total_anomaly_count)\n",
    "    contextual_budget = int(0.3 * total_anomaly_count)\n",
    "    collective_budget = total_anomaly_count - point_budget - contextual_budget\n",
    "\n",
    "    # --- Point anomalies ---\n",
    "    indices_point = np.random.choice(n, size=point_budget, replace=False)\n",
    "    data[indices_point] += np.random.normal(\n",
    "        loc=1.5 * data_mean, scale=0.3 * data_std, size=point_budget\n",
    "    )\n",
    "    anomaly_labels[indices_point] = 1\n",
    "    anomaly_types[indices_point] = \"point\"\n",
    "\n",
    "    # --- Contextual anomalies ---\n",
    "    window_size = 12\n",
    "    indices_contextual = np.random.choice(\n",
    "        range(window_size, n - window_size), size=contextual_budget, replace=False\n",
    "    )\n",
    "    for idx in indices_contextual:\n",
    "        local_mean = np.mean(data[idx - window_size : idx])\n",
    "        data[idx] = local_mean + np.random.normal(\n",
    "            loc=0.8 * data_std, scale=0.2 * data_std\n",
    "        )\n",
    "        anomaly_labels[idx] = 1\n",
    "        anomaly_types[idx] = \"contextual\"\n",
    "\n",
    "    # --- Collective anomalies ---\n",
    "    collective_seq_length = 5\n",
    "    collective_count = collective_budget // collective_seq_length\n",
    "    indices_collective = np.random.choice(\n",
    "        range(n - collective_seq_length), size=collective_count, replace=False\n",
    "    )\n",
    "    for idx in indices_collective:\n",
    "        anomaly_sequence = np.random.normal(\n",
    "            loc=data_mean + 1.0 * data_std, scale=0.3 * data_std, size=5\n",
    "        )\n",
    "        data[idx : idx + 5] = anomaly_sequence\n",
    "        anomaly_labels[idx : idx + 5] = 1\n",
    "        anomaly_types[idx : idx + 5] = \"collective\"\n",
    "\n",
    "    return original_data, data, anomaly_labels, anomaly_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f19d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anomalies_with_colors(original_data, injected_data, types):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot original data line\n",
    "    plt.plot(\n",
    "        original_data,\n",
    "        label=\"Original Test Data\",\n",
    "        color=\"green\",\n",
    "        alpha=0.9,\n",
    "        linewidth=1.2,\n",
    "        zorder=1,\n",
    "    )\n",
    "\n",
    "    # Highlight only anomaly points with scatter plots in different colors\n",
    "    anomaly_colors = {\"point\": \"red\", \"contextual\": \"orange\", \"collective\": \"pink\"}\n",
    "\n",
    "    for anomaly_type, color in anomaly_colors.items():\n",
    "        indices = np.where(types == anomaly_type)[0]\n",
    "        if len(indices) > 0:\n",
    "            plt.scatter(\n",
    "                indices,\n",
    "                injected_data[indices],\n",
    "                label=f\"{anomaly_type.capitalize()} Anomaly\",\n",
    "                color=color,\n",
    "                s=50,\n",
    "                linewidths=0.7,\n",
    "                edgecolors=\"black\",\n",
    "                zorder=3,\n",
    "            )\n",
    "\n",
    "    plt.title(\"ETTh1 - Test Data with Injected Anomalies Highlighted\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../data/processed/etth1_test.csv\", parse_dates=[\"date\"], index_col=\"date\")\n",
    "\n",
    "raw_test = test_df[\"OT\"].values.copy()\n",
    "\n",
    "#inject anomalies into the test set\n",
    "original_test_data, test_data_with_anomalies, anomaly_labels, anomaly_types = inject_anomalies(raw_test.copy(), anomaly_fraction=0.05)\n",
    "\n",
    "plot_anomalies_with_colors(original_test_data, test_data_with_anomalies, anomaly_types)\n",
    "\n",
    "# Save the modified test data with anomalies\n",
    "np.save(\"../data/processed/etth1_test_with_anomalies.npy\", test_data_with_anomalies)\n",
    "\n",
    "# Save the anomaly labels\n",
    "np.save(\"../data/processed/etth1_anomaly_labels.npy\", anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64826205",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a new DataLoader for the modified test set\n",
    "test_loader_with_anomalies = DataLoader(TimeSeriesDataset(test_data_with_anomalies, window_size), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate the model on the modified test set with anomalies\n",
    "preds_with_anomalies, trues_with_anomalies = evaluate_model(model, test_loader_with_anomalies, \"Test Set with Anomalies Prediction vs Ground Truth\", \"lstm_model_test_with_anomalies_predictions.png\", writer, step=2)\n",
    "\n",
    "\n",
    "# isolation forest anomaly detection on the modified test set\n",
    "# convert preds_with_anomalies and trues_with_anomalies to 1D array\n",
    "if preds_with_anomalies.ndim > 1:\n",
    "    preds_with_anomalies = preds_with_anomalies.squeeze()\n",
    "if trues_with_anomalies.ndim > 1:\n",
    "    trues_with_anomalies = trues_with_anomalies.squeeze()\n",
    "# calculate residuals\n",
    "residuals_with_anomalies = np.abs(trues_with_anomalies - preds_with_anomalies)\n",
    "# Isolation Forest anomaly detection on the modified test set\n",
    "iso_anomalies_with_anomalies = isolation_forest_detection(residuals_with_anomalies)\n",
    "# Visualize Isolation Forest anomalies on the modified test set\n",
    "visualize_isolation_forest_anomalies(trues_with_anomalies, preds_with_anomalies, iso_anomalies_with_anomalies)\n",
    "\n",
    "\n",
    "# get true targets (shifted by window size)\n",
    "trues_targets = test_data_with_anomalies[window_size:]\n",
    "true_labels = anomaly_labels[window_size:]\n",
    "\n",
    "# residuals\n",
    "\n",
    "residuals=np.abs(preds_with_anomalies - trues_targets)\n",
    "\n",
    "# ROC AUC\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(true_labels, residuals)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(f\"ROC AUC Score on Modified Test Set: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot ROC Curve for modified test set\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label=\"Random Guess\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve for LSTM Anomaly Detection on Modified Test Set\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../../outputs/LSTM_Model/roc_curve_modified_test_set.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f98f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation of precision, recall and f1-score\n",
    "\n",
    "# Set threshold using top 5% of residuals\n",
    "threshold = np.percentile(residuals, 95)\n",
    "\n",
    "# Predict anomalies using threshold\n",
    "predicted_labels = (residuals > threshold).astype(int)\n",
    "\n",
    "# Precision, Recall, F1\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a088cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Search for Best Threshold ===\n",
    "best_f1 = 0\n",
    "best_thresh = 0\n",
    "thresholds = np.linspace(residuals.min(), residuals.max(), 500)\n",
    "\n",
    "for thresh in thresholds:\n",
    "    preds = (residuals > thresh).astype(int)\n",
    "    f1 = f1_score(true_labels, preds)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thresh = thresh\n",
    "\n",
    "print(f\"‚úÖ Best Threshold: {best_thresh:.4f}\")\n",
    "print(f\"‚úÖ Max F1 Score:  {best_f1:.4f}\")\n",
    "\n",
    "# === 2. Use Best Threshold to Classify ===\n",
    "predicted_labels = (residuals > best_thresh).astype(int)\n",
    "\n",
    "# === 3. Compute Final Metrics ===\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "\n",
    "# === 4. Plot Precision-Recall Curve ===\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(true_labels, residuals)\n",
    "ap_score = average_precision_score(true_labels, residuals)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_vals, precision_vals, color='darkorange', lw=2, label=f'AP={ap_score:.4f}')\n",
    "plt.axvline(x=recall, color='gray', linestyle='--', label=f'Recall@Best={recall:.2f}')\n",
    "plt.axhline(y=precision, color='gray', linestyle='--', label=f'Precision@Best={precision:.2f}')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve (Residuals as Scores)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === 5. Plot Confusion Matrix ===\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Normal\", \"Anomaly\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "disp.plot(ax=ax, cmap=\"Blues\", values_format='d')\n",
    "plt.title(f\"Confusion Matrix @ Best Threshold ({best_thresh:.4f})\")\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
